#!/usr/bin/env python3
"""
svm_knn_trainer_fast.py (Fixed + Enhanced)

- Adaptive CV: auto-adjusts splits when classes are tiny to avoid
  "n_splits cannot be greater than the number of members in each class".
- Prints and saves BOTH in-sample (train) and out-sample (test) accuracies.
- RandomizedSearchCV for SVM (rbf/poly-3) and KNN.
- Train-only feature pruning (low-variance + high-corr).
- Optional time-series CV and label derivation from price.

Outputs (in --outdir, default 'artifacts'):
  - SVM_report.txt, KNN_report.txt
  - SVM_confusion_matrix.csv, KNN_confusion_matrix.csv
  - meta.json (with train/test accuracies + best params)
  - summary.txt (2 lines you can paste into your report)
"""

import argparse, json, os
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.model_selection import (
    StratifiedKFold, TimeSeriesSplit, RandomizedSearchCV,
    StratifiedShuffleSplit
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC

# ---------------- CLI ----------------
def parse_args(argv=None):
    ap = argparse.ArgumentParser(description="Fast SVM & KNN trainer with randomized search.", add_help=True)
    ap.add_argument("--x-train", default="X_train.csv")
    ap.add_argument("--x-test",  default="X_test.csv")
    ap.add_argument("--y-train", default="y_train.csv")
    ap.add_argument("--y-test",  default="y_test.csv")
    ap.add_argument("--y-train-col", default=None)
    ap.add_argument("--y-test-col",  default=None)

    # derive labels when y_* unusable
    ap.add_argument("--derive-labels", action="store_true")
    ap.add_argument("--price-col", default=None)
    ap.add_argument("--horizon", type=int, default=5)
    ap.add_argument("--three-class", action="store_true")

    # pruning
    ap.add_argument("--corr-threshold", type=float, default=0.98)
    ap.add_argument("--min-std", type=float, default=0.0)

    # CV style
    ap.add_argument("--cv", choices=["stratified", "timeseries"], default="stratified")

    # scoring
    ap.add_argument("--scoring", default=None)

    # speed flags
    ap.add_argument("--fast", action="store_true")
    ap.add_argument("--probabilities", action="store_true")

    ap.add_argument("--outdir", default="artifacts")

    if argv is None:
        args, _ = ap.parse_known_args()
    else:
        args, _ = ap.parse_known_args(argv)
    return args

# ---------------- helpers ----------------
DATE_HINTS = ("date","time","timestamp","day")
PRICE_CANDIDATES = ["Adj Close","Adj_Close","adj_close","Close","close"]

def is_date_like(s: pd.Series) -> bool:
    name = (s.name or "").lower()
    if any(k in name for k in DATE_HINTS): return True
    try:
        parsed = pd.to_datetime(s, errors="coerce")
        return parsed.notna().mean() > 0.6
    except Exception:
        return False

def pick_label_col(df: pd.DataFrame):
    # try preferred names first
    for p in ["y","label","class","target","signal"]:
        for c in df.columns:
            if c.lower()==p: return c
    # else first non-date-like column
    for c in df.columns:
        if not is_date_like(df[c]): return c
    return df.columns[0]

def select_numeric(Xtr, Xte):
    Xtrn = Xtr.select_dtypes(include=["number"]).copy()
    Xten = Xte.select_dtypes(include=["number"]).copy()
    common = [c for c in Xtrn.columns if c in Xten.columns]
    if not common:
        raise ValueError("No overlapping numeric feature columns between X_train and X_test.")
    return Xtrn[common], Xten[common], common

def prune_features_train_only(Xtr_num: pd.DataFrame, Xte_num: pd.DataFrame, min_std: float, corr_thr: float):
    stds = Xtr_num.std(ddof=0)
    keep = stds[stds > min_std].index.tolist()
    Xtr_f = Xtr_num[keep].copy()
    Xte_f = Xte_num[keep].copy()

    if Xtr_f.shape[1] > 1:
        corr = Xtr_f.corr().abs()
        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
        to_drop = [col for col in upper.columns if (upper[col] >= corr_thr).any()]
    else:
        to_drop = []
    kept = [c for c in Xtr_f.columns if c not in to_drop]
    return Xtr_f[kept], Xte_f[kept], kept, {
        "dropped_low_variance": [c for c in Xtr_num.columns if c not in keep],
        "dropped_high_corr": to_drop
    }

def find_price_col(df: pd.DataFrame, override=None) -> str:
    if override:
        if override not in df.columns: raise ValueError(f"--price-col '{override}' not in X.")
        return override
    for c in PRICE_CANDIDATES:
        if c in df.columns: return c
    for c in df.columns:
        if "close" in c.lower(): return c
    raise ValueError("No price column found.")

def derive_labels_from_price(Xtr_raw, Xte_raw, price_col, H=5, three_class=False):
    Xtr = Xtr_raw.copy(); Xte = Xte_raw.copy()
    def fwd_ret(x, h): return x.shift(-h)/x - 1.0
    ytr_c = fwd_ret(Xtr[price_col], H)
    yte_c = fwd_ret(Xte[price_col], H)
    vt, ve = ytr_c.notna(), yte_c.notna()
    Xtr, Xte = Xtr.loc[vt], Xte.loc[ve]
    ytr_c, yte_c = ytr_c.loc[vt], yte_c.loc[ve]
    if three_class:
        q1, q3 = ytr_c.quantile([0.25, 0.75])
        def terc(r): return -1 if r <= q1 else (1 if r >= q3 else 0)
        ytr = ytr_c.apply(terc); yte = yte_c.apply(terc)
    else:
        ytr = (ytr_c > 0).astype(int); yte = (yte_c > 0).astype(int)
    return Xtr, Xte, ytr, yte

# ---------------- adaptive CV ----------------
def choose_cv(cv_type: str, ytr: pd.Series, n_samples: int):
    """
    Returns a CV splitter that won't error on small classes.
    Prints class counts and chosen CV for transparency.
    """
    ytr = pd.Series(ytr)
    if cv_type == "timeseries":
        n_splits = 3 if n_samples >= 3 else 2
        print(f"[CV] TimeSeriesSplit with n_splits={n_splits} (n_samples={n_samples})")
        return TimeSeriesSplit(n_splits=n_splits)

    # Stratified path
    counts = ytr.value_counts().sort_index()
    min_count = int(counts.min()) if len(counts) else 0
    print(f"[CV] Stratified CV; class counts: {counts.to_dict()} (min={min_count})")

    if min_count >= 3:
        print("[CV] Using StratifiedKFold(n_splits=3)")
        return StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    elif min_count == 2:
        print("[CV] Using StratifiedKFold(n_splits=2) due to small class")
        return StratifiedKFold(n_splits=2, shuffle=True, random_state=42)
    else:
        # some class has only 1 sample; use shuffle split
        print("[CV] Using StratifiedShuffleSplit(n_splits=3, test_size=0.25) due to a singleton class")
        return StratifiedShuffleSplit(n_splits=3, test_size=0.25, random_state=42)

# ---------------- training ----------------
def fit_eval_fast(Xtr_num, Xte_num, ytr, yte, feature_cols, scoring, outdir, cv_type, probs):
    n_classes = len(np.unique(ytr))
    scoring = scoring or ("f1_macro" if n_classes > 2 else "f1")

    num_pipe = Pipeline([("imputer", SimpleImputer(strategy="median")),
                         ("scaler", StandardScaler())])
    preprocess = ColumnTransformer([("num", num_pipe, feature_cols)], remainder="drop")

    # Adaptive CV
    cv = choose_cv(cv_type, ytr, n_samples=len(Xtr_num))

    # ---------- SVM ----------
    from scipy.stats import loguniform
    svm = Pipeline([("prep", preprocess), ("clf", SVC(probability=probs))])
    svm_search_space = [
        {"clf__kernel": ["rbf"],
         "clf__C": loguniform(1e-1, 1e1),
         "clf__gamma": ["scale", 1e-2, 5e-2],
         "clf__class_weight": [None, "balanced"]},
        {"clf__kernel": ["poly"], "clf__degree": [3],
         "clf__C": loguniform(1e-1, 1e1),
         "clf__gamma": ["scale", 1e-2],
         "clf__coef0": [0.0, 1.0],
         "clf__class_weight": [None, "balanced"]}
    ]
    svm_rs = RandomizedSearchCV(
        svm, svm_search_space, n_iter=12, cv=cv, scoring=scoring,
        n_jobs=-1, refit=True, verbose=0, random_state=13
    )

    # ---------- KNN ----------
    knn = Pipeline([("prep", preprocess), ("clf", KNeighborsClassifier())])
    knn_search_space = {
        "clf__n_neighbors": [11, 31, 51, 101],
        "clf__weights": ["uniform", "distance"],
        "clf__p": [1, 2],
        "clf__leaf_size": [20, 30, 40]
    }
    knn_rs = RandomizedSearchCV(
        knn, knn_search_space, n_iter=10, cv=cv, scoring=scoring,
        n_jobs=-1, refit=True, verbose=0, random_state=13
    )

    print(f"[INFO] RandomizedSearchCV (SVM) with {svm_rs.n_iter} samples; scoring={scoring}")
    svm_rs.fit(Xtr_num, ytr)
    print(f"[INFO] RandomizedSearchCV (KNN) with {knn_rs.n_iter} samples; scoring={scoring}")
    knn_rs.fit(Xtr_num, ytr)

    def evaluate(model, name):
        # Out-sample
        yhat_test = model.predict(Xte_num)
        acc_test = accuracy_score(yte, yhat_test)
        bal_test = balanced_accuracy_score(yte, yhat_test)
        f1m_test = f1_score(yte, yhat_test, average="macro" if n_classes>2 else "binary")

        # In-sample
        yhat_train = model.predict(Xtr_num)
        acc_train = accuracy_score(ytr, yhat_train)
        bal_train = balanced_accuracy_score(ytr, yhat_train)

        # Persist artifacts
        os.makedirs(outdir, exist_ok=True)
        pd.DataFrame({"y_true": yte, "y_pred": yhat_test}).to_csv(os.path.join(outdir, f"{name}_predictions.csv"), index=False)
        with open(os.path.join(outdir, f"{name}_report.txt"), "w") as f:
            f.write(classification_report(yte, yhat_test, digits=3))
        pd.DataFrame(confusion_matrix(yte, yhat_test)).to_csv(os.path.join(outdir, f"{name}_confusion_matrix.csv"), index=False)

        # Console summary
        print("\n" + "="*80)
        print(f"{name} best_params: {model.best_params_}")
        print(f"In-sample accuracy={acc_train:.4f} | Out-sample accuracy={acc_test:.4f} | BalancedAcc(test)={bal_test:.4f} | F1(test)={f1m_test:.4f}")

        return {
            "best_params": model.best_params_,
            "train_accuracy": acc_train,
            "test_accuracy": acc_test,
            "train_bal_acc": bal_train,
            "test_bal_acc": bal_test,
            "test_f1": f1m_test
        }

    svm_metrics = evaluate(svm_rs, "SVM")
    knn_metrics = evaluate(knn_rs, "KNN")
    return svm_metrics, knn_metrics

# ---------------- main ----------------
def main(argv=None):
    args = parse_args(argv)

    # load X
    Xtr_raw = pd.read_csv(args.x_train)
    Xte_raw = pd.read_csv(args.x_test)

    # labels
    use_derivation = args.derive_labels
    if not use_derivation:
        try:
            ytr_df = pd.read_csv(args.y_train); yte_df = pd.read_csv(args.y_test)
            ytr_col = args.y_train_col or pick_label_col(ytr_df)
            yte_col = args.y_test_col  or pick_label_col(yte_df)
            if (ytr_col is None) or (yte_col is None) or is_date_like(ytr_df[ytr_col]) or is_date_like(yte_df[yte_col]):
                print("[WARN] y_* unusable (likely dates). Deriving labels from X.")
                use_derivation = True
            else:
                ytr = ytr_df[ytr_col]; yte = yte_df[yte_col]
                XtrL, XteL = Xtr_raw.copy(), Xte_raw.copy()
        except Exception:
            print("[WARN] Failed to parse y_*; deriving labels from X.")
            use_derivation = True

    if use_derivation:
        price_col = find_price_col(Xtr_raw, args.price_col)
        if price_col not in Xte_raw.columns:
            raise ValueError(f"Price column '{price_col}' not found in X_test.")
        XtrL, XteL, ytr, yte = derive_labels_from_price(
            Xtr_raw, Xte_raw, price_col=price_col, H=args.horizon, three_class=args.three_class
        )

    # numeric + pruning
    Xtr_num, Xte_num, feature_cols = select_numeric(XtrL, XteL)
    Xtr_num, Xte_num, kept_cols, prune_meta = prune_features_train_only(
        Xtr_num, Xte_num, min_std=args.min_std, corr_thr=args.corr_threshold
    )

    # label encoding if needed
    if ytr.dtype == "O" or yte.dtype == "O":
        le = LabelEncoder()
        ytr = pd.Series(le.fit_transform(ytr), name="y")
        unknown = set(yte.unique()) - set(le.classes_)
        if unknown:
            raise ValueError(f"y_test contains unseen labels vs y_train (e.g., {list(sorted(unknown))[:5]})")
        yte = pd.Series(le.transform(yte), name="y")

    # train & eval
    svm_metrics, knn_metrics = fit_eval_fast(
        Xtr_num, Xte_num, ytr, yte, kept_cols,
        scoring=args.scoring, outdir=args.outdir,
        cv_type=("timeseries" if args.cv=="timeseries" else "stratified"),
        probs=args.probabilities
    )

    # meta + summary
    meta = {
        "derived_labels": use_derivation,
        "cv": args.cv,
        "horizon": args.horizon,
        "three_class": args.three_class,
        "feature_count_before": len(feature_cols),
        "feature_count_after": len(kept_cols),
        "dropped_low_variance": prune_meta["dropped_low_variance"],
        "dropped_high_corr": prune_meta["dropped_high_corr"],
        "feature_sample": kept_cols[:20],
        "svm": svm_metrics,
        "knn": knn_metrics,
    }
    os.makedirs(args.outdir, exist_ok=True)
    meta_path = os.path.join(args.outdir, "meta.json")
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)

    # Write 2-line report summary
    summary_lines = [
        f"For SVM my in-sample accuracy: {svm_metrics['train_accuracy']*100:.2f}% and my out-sample accuracy: {svm_metrics['test_accuracy']*100:.2f}%.",
        f"For KNN my in-sample accuracy: {knn_metrics['train_accuracy']*100:.2f}% and my out-sample accuracy: {knn_metrics['test_accuracy']*100:.2f}%."
    ]
    with open(os.path.join(args.outdir, "summary.txt"), "w") as f:
        f.write("\n".join(summary_lines))

    print("\n[DONE] Results saved in:", os.path.abspath(args.outdir))
    print("\nSummary (copy/paste these lines):")
    for line in summary_lines:
        print("  " + line)

if __name__ == "__main__":
    main()
